<div id="quesUser" class="depot_container">
    <div class="question_depict"><a href="/question/index/type/1" class="question_iconwrap"><i
                class="question_icon"></i>返回
        </a>
        <h3>机器学习</h3>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">1. </span>
        <pre class="question_main">请详细说说支持向量机（support vector machine，SVM）的原理</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    136123&nbsp;浏览
                </li>
                <li>
                    28&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">2. </span>
        <pre class="question_main">哪些机器学习算法不需要做归一化处理？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    89203&nbsp;浏览
                </li>
                <li>
                    35&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">3. </span>
        <pre class="question_main">树形结构为什么不需要归一化？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    68947&nbsp;浏览
                </li>
                <li>
                    15&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">4. </span>
        <pre class="question_main">在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    61203&nbsp;浏览
                </li>
                <li>
                    7&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">5. </span>
        <pre class="question_main">数据归一化（或者标准化，注意归一化和标准化不同）的原因</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    53120&nbsp;浏览
                </li>
                <li>
                    12&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">6. </span>
        <pre class="question_main">请简要说说一个完整机器学习项目的流程</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    52745&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">7. </span>
        <pre class="question_main">逻辑斯蒂回归为什么要对特征进行离散化。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    48501&nbsp;浏览
                </li>
                <li>
                    15&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">8. </span>
        <pre class="question_main">简单介绍下LR</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    48436&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">9. </span>
        <pre class="question_main">overfitting怎么解决</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    44345&nbsp;浏览
                </li>
                <li>
                    12&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">10. </span>
        <pre class="question_main">LR和SVM的联系与区别</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    42864&nbsp;浏览
                </li>
                <li>
                    20&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">11. </span>
        <pre class="question_main">什么是熵</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    37965&nbsp;浏览
                </li>
                <li>
                    10&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">12. </span>
        <pre class="question_main">说说梯度下降法</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    37965&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">13. </span>
        <pre class="question_main">牛顿法和梯度下降法有什么不同？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    37206&nbsp;浏览
                </li>
                <li>
                    7&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">14. </span>
        <pre class="question_main">熵、联合熵、条件熵、相对熵、互信息的定义</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    33135&nbsp;浏览
                </li>
                <li>
                    8&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">15. </span>
        <pre class="question_main">说说你知道的核函数</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    33995&nbsp;浏览
                </li>
                <li>
                    12&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">16. </span>
        <pre class="question_main">什么是拟牛顿法（Quasi-Newton Methods）？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    31175&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">17. </span>
        <pre class="question_main">kmeans的复杂度？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    31870&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">18. </span>
        <pre class="question_main">请说说随机梯度下降法的问题和挑战？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    30404&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">19. </span>
        <pre class="question_main">说说共轭梯度法？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    28262&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">20. </span>
        <pre class="question_main">对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    28692&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">21. </span>
        <pre class="question_main">什么是最大熵</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    29494&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">22. </span>
        <pre class="question_main">LR与线性回归的区别与联系</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    30493&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">23. </span>
        <pre class="question_main">简单说下有监督学习和无监督学习的区别</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    26732&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">24. </span>
        <pre class="question_main">机器学习中的正则化到底是什么意思？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    29568&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">25. </span>
        <pre class="question_main">说说常见的损失函数？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    27755&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">26. </span>
        <pre class="question_main">为什么xgboost要用泰勒展开，优势在哪里？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    29686&nbsp;浏览
                </li>
                <li>
                    7&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">27. </span>
        <pre class="question_main">协方差和相关性有什么区别？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    24069&nbsp;浏览
                </li>
                <li>
                    7&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">28. </span>
        <pre class="question_main">xgboost如何寻找最优特征？是有放回还是无放回的呢？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    25563&nbsp;浏览
                </li>
                <li>
                    7&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">29. </span>
        <pre class="question_main">谈谈判别式模型和生成式模型？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    24146&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">30. </span>
        <pre class="question_main">线性分类器与非线性分类器的区别以及优劣</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    23128&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">31. </span>
        <pre class="question_main">L1和L2的区别</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    25224&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">32. </span>
        <pre class="question_main">L1和L2正则先验分别服从什么分布</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    24460&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">33. </span>
        <pre class="question_main">简单介绍下logistics回归？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    23334&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">34. </span>
        <pre class="question_main">说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    22757&nbsp;浏览
                </li>
                <li>
                    10&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">35. </span>
        <pre class="question_main">经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：<img src="http://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512809753_997.png">
这叫做拼写检查。

根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    23003&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">36. </span>
        <pre class="question_main">为什么朴素贝叶斯如此“朴素”？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    23799&nbsp;浏览
                </li>
                <li>
                    8&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">37. </span>
        <pre class="question_main">请大致对比下plsa和LDA的区别</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    21016&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">38. </span>
        <pre class="question_main">请详细说说EM算法</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    24534&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">39. </span>
        <pre class="question_main">KNN中的K如何选取的？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    22678&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">40. </span>
        <pre class="question_main">防止过拟合的方法</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    21657&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">41. </span>
        <pre class="question_main">机器学习中，为何要经常对数据做归一化</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    22918&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">42. </span>
        <pre class="question_main">什么最小二乘法？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    20420&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">43. </span>
        <pre class="question_main">梯度下降法找到的一定是下降最快的方向么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    20817&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">44. </span>
        <pre class="question_main">简单说说贝叶斯定理</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    19975&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">45. </span>
        <pre class="question_main">怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    21996&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">46. </span>
        <pre class="question_main">请举例说明什么是标准化、归一化</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    20642&nbsp;浏览
                </li>
                <li>
                    11&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">47. </span>
        <pre class="question_main">随机森林如何处理缺失值？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    22082&nbsp;浏览
                </li>
                <li>
                    11&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">48. </span>
        <pre class="question_main">随机森林如何评估特征重要性？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    20152&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">49. </span>
        <pre class="question_main">请说说Kmeans的优化？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    19937&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">50. </span>
        <pre class="question_main">KMeans算法k值及初始类簇中心点的选取。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    19518&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">51. </span>
        <pre class="question_main">解释对偶的概念。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    18874&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">52. </span>
        <pre class="question_main">如何进行特征选择？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    19042&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">53. </span>
        <pre class="question_main">衡量分类器的好坏？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    18092&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">54. </span>
        <pre class="question_main">机器学习和统计里面的auc的物理意义是啥？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    19501&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">55. </span>
        <pre class="question_main">数据预处理。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    17877&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">56. </span>
        <pre class="question_main">观察增益gain, alpha和gamma越大，增益越小？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    16803&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">57. </span>
        <pre class="question_main">什麽造成梯度消失问题?</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    18002&nbsp;浏览
                </li>
                <li>
                    7&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">58. </span>
        <pre class="question_main">到底什么是特征工程？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    19517&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">59. </span>
        <pre class="question_main">你知道有哪些数据处理和特征工程的处理？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    17363&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">60. </span>
        <pre class="question_main">准备机器学习面试应该了解哪些理论知识？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    18106&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">61. </span>
        <pre class="question_main">数据不平衡问题</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    18156&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">62. </span>
        <pre class="question_main">特征比数据量还大时，选择什么样的分类器？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    17328&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">63. </span>
        <pre class="question_main">常见的分类算法有哪些？他们各自的优缺点是什么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    16977&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">64. </span>
        <pre class="question_main">常见的监督学习算法有哪些？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15597&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">65. </span>
        <pre class="question_main">说说常见的优化算法及其优缺点？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    17155&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">66. </span>
        <pre class="question_main">特征向量的归一化方法有哪些？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15333&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">67. </span>
        <pre class="question_main">RF与GBDT之间的区别与联系？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    16456&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">68. </span>
        <pre class="question_main">试证明样本空间中任意点 x 到超平面 (w,b) 的距离公式.

<img src="http://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584758237312759.png">

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15058&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">69. </span>
        <pre class="question_main">请比较下EM算法、HMM、CRF</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15854&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">70. </span>
        <pre class="question_main">带核的SVM为什么能分类非线性问题？ </pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15928&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">71. </span>
        <pre class="question_main">请说说常用核函数及核函数的条件</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15731&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">72. </span>
        <pre class="question_main">请具体说说Boosting和Bagging的区别</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    16181&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">73. </span>
        <pre class="question_main">逻辑回归相关问题</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    14963&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">74. </span>
        <pre class="question_main">什么是共线性, 跟过拟合有什么关联?</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    14679&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">75. </span>
        <pre class="question_main">机器学习中，有哪些特征选择的工程方法？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    14645&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">76. </span>
        <pre class="question_main">用贝叶斯机率说明Dropout的原理</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    14772&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">77. </span>
        <pre class="question_main">对于维度极低的特征，选择线性还是非线性分类器？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    13881&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">78. </span>
        <pre class="question_main">请问怎么处理特征向量的缺失值</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    13547&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">79. </span>
        <pre class="question_main">SVM、LR、决策树的对比。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15301&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">80. </span>
        <pre class="question_main">简述KNN最近邻分类算法的过程？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    13960&nbsp;浏览
                </li>
                <li>
                    7&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">81. </span>
        <pre class="question_main">常用的聚类划分方式有哪些？列举代表算法。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    14319&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">82. </span>
        <pre class="question_main">什么是偏差与方差？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    14204&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">83. </span>
        <pre class="question_main">解决bias和Variance问题的方法是什么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    14182&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">84. </span>
        <pre class="question_main">采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    13643&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">85. </span>
        <pre class="question_main">xgboost怎么给特征评分？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    15256&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">86. </span>
        <pre class="question_main">什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    13928&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">87. </span>
        <pre class="question_main">推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到</pre>
        <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    12078&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">88. </span>
        <pre class="question_main">请写出你了解的机器学习特征工程操作，以及它的意义</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    12798&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">89. </span>
        <pre class="question_main">请写出你对VC维的理解和认识</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    11023&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">90. </span>
        <pre class="question_main">kmeans聚类中，如何确定k的大小</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    12034&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">91. </span>
        <pre class="question_main">请用Python实现下线性回归，并思考下更高效的实现方式</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    10717&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">92. </span>
        <pre class="question_main">怎么理解“机器学习的各种模型与他们各自的损失函数一一对应？”</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9791&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">93. </span>
        <pre class="question_main">给你一个有1000列和1百万行的训练数据集。这个数据集是基于分类问题的。
经理要求你来降低该数据集的维度以减少模型计算时间。你的机器内存有限。你会怎么做？（你可以自由做各种实际操作假设）</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9962&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">94. </span>
        <pre class="question_main">问2：在PCA中有必要做旋转变换吗？
如果有必要，为什么？如果你没有旋转变换那些成分，会发生什么情况？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9834&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">95. </span>
        <pre class="question_main">给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9027&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">96. </span>
        <pre class="question_main">给你一个癌症检测的数据集。你已经建好了分类模型，取得了96％的精度。为什么你还是不满意你的模型性能？你可以做些什么呢？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9880&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">97. </span>
        <pre class="question_main">解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9518&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">98. </span>
        <pre class="question_main">你正在一个时间序列数据集上工作。经理要求你建立一个高精度的模型。你开始用决策树算法，因为你知道它在所有类型数据上的表现都不错。
后来，你尝试了时间序列回归模型，并得到了比决策树模型更高的精度。

这种情况会发生吗？为什么？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9789&nbsp;浏览
                </li>
                <li>
                    8&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">99. </span>
        <pre class="question_main">给你分配了一个新的项目，是关于帮助食品配送公司节省更多的钱。问题是，公司的送餐队伍没办法准时送餐。结果就是他们的客户很不高兴。

最后为了使客户高兴，他们只好以免餐费了事。哪个机器学习算法能拯救他们？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9764&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">100. </span>
        <pre class="question_main">你意识到你的模型受到低偏差和高方差问题的困扰。应该使用哪种算法来解决问题呢？为什么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9450&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">101. </span>
        <pre class="question_main">给你一个数据集。该数据集包含很多变量，你知道其中一些是高度相关的。&lt;br/&gt;经理要求你用PCA。你会先去掉相关的变量吗？为什么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9320&nbsp;浏览
                </li>
                <li>
                    9&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">102. </span>
        <pre class="question_main">花了几个小时后，现在你急于建一个高精度的模型。结果，你建了5 个GBM （Gradient Boosted Models），想着boosting算法会显示魔力。
不幸的是，没有一个模型比基准模型表现得更好。最后，你决定将这些模型结合到一起。
尽管众所周知，结合模型通常精度高，但你就很不幸运。你到底错在哪里？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9322&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">103. </span>
        <pre class="question_main">KNN和KMEANS聚类（kmeans clustering）有什么不同？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9663&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">104. </span>
        <pre class="question_main">真阳性率和召回有什么关系？写出方程式。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9169&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">105. </span>
        <pre class="question_main">在分析了你的模型后，经理告诉你，你的模型有多重共线性。
你会如何验证他说的是真的？在不丢失任何信息的情况下，你还能建立一个更好的模型吗？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8742&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">106. </span>
        <pre class="question_main">什么时候Ridge回归优于Lasso回归？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    10293&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">107. </span>
        <pre class="question_main">如何在一个数据集上选择重要的变量？给出解释。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8591&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">108. </span>
        <pre class="question_main">Gradient boosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9983&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">109. </span>
        <pre class="question_main">运行二元分类树算法很容易，但是你知道一个树是如何做分割的吗，即树如何决定把哪些变量分到哪个根节点和后续节点上？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    7955&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">110. </span>
        <pre class="question_main">你有一个数据集，变量个数p大于观察值个数n。为什么用OLS是一个不好的选择？用什么技术最好？为什么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    7991&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">111. </span>
        <pre class="question_main">什么是凸包？（提示：想一想SVM）
其他方法还包括子集回归、前向逐步回归。</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8027&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">112. </span>
        <pre class="question_main">我们知道，独热编码（OneHotEncoder）会增加数据集的维度。但是标签编码（LabelEncoder）不会。为什么？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8045&nbsp;浏览
                </li>
                <li>
                    5&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">113. </span>
        <pre class="question_main">你会在时间序列数据集上使用什么交叉验证技术？是用k倍或LOOCV？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    7798&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">114. </span>
        <pre class="question_main">给你一个缺失值多于30%的数据集？比方说，在50个变量中，有8个变量的缺失值都多于30%。你对此如何处理？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8634&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">115. </span>
        <pre class="question_main">“买了这个的客户，也买了......”亚马逊的建议是哪种算法的结果？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9272&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">116. </span>
        <pre class="question_main">你怎么理解第一类和第二类错误？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8714&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">117. </span>
        <pre
            class="question_main">当你在解决一个分类问题时，出于验证的目的，你已经将训练集随机抽样地分成训练集和验证集。你对你的模型能在未看见的数据上有好的表现非常有信心，因为你的验证精度高。但是，在得到很差的精度后，你大失所望。什么地方出了错？</pre>
        <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8325&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">118. </span>
        <pre class="question_main">请简单阐述下决策树、回归、SVM、神经网络等算法各自的优缺点？

正则化算法（Regularization Algorithms）
集成算法（Ensemble Algorithms）

决策树算法（Decision Tree Algorithm）
回归（Regression）
人工神经网络（Artificial Neural Network）
深度学习（Deep Learning）
支持向量机（Support Vector Machine）
降维算法（Dimensionality Reduction Algorithms）

聚类算法（Clustering Algorithms）
基于实例的算法（Instance-based Algorithms）
贝叶斯算法（Bayesian Algorithms）
关联规则学习算法（Association Rule Learning Algorithms）
图模型（Graphical Models）</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    13910&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">119. </span>
        <pre class="question_main">在应用机器学习算法之前纠正和清理数据的步骤是什么？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8049&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">120. </span>
        <pre class="question_main">什么是K-means聚类算法？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8628&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">121. </span>
        <pre class="question_main">如何理解模型的过拟合与欠拟合，以及如何解决？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8406&nbsp;浏览
                </li>
                <li>
                    6&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">122. </span>
        <pre class="question_main">请详细说说文字特征提取</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    7850&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">123. </span>
        <pre class="question_main">请详细说说图像特征提取</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8097&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">124. </span>
        <pre class="question_main">了解xgboost么，请详细说说它的原理</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    21716&nbsp;浏览
                </li>
                <li>
                    8&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">125. </span>
        <pre class="question_main">请详细说说梯度提升树(GBDT)的原理</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    10313&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">126. </span>
        <pre class="question_main">请说说Adaboost 算法的原理与推导</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    7943&nbsp;浏览
                </li>
                <li>
                    3&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">127. </span>
        <pre class="question_main">机器学习中的L0、L1与L2范数到底是什么意思？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8309&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">128. </span>
        <pre class="question_main">请详细说说决策树的构造原理
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    5638&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">129. </span>
        <pre class="question_main">怎么确定LDA的topic个数？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    5962&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">130. </span>
        <pre class="question_main">sklearn随机森林的特征重要度是不是偏好数值型变量呢？

我在做kaggle的Titanic问题时使用随机森林和xgboost发现两个数值型的变量重要度非常高，远远高过性别这种在数据分析时候认为很重要的特征
看sklearn文档说特征重要度是按照特征对不纯度减少的贡献来排的，刚才在网上找到了一篇论文大概是说这种特征重要度的衡量方式会偏好那些类别多的变量（feature selection based on impurity reduction is biased towards preferring variables with more categories）。

sklearn的文档说sklearn的决策树都是cart树，cart树在对待数值型特征的时候也可以理解成一个类别数等于样本数的类别型特征吧。那么是因为这个原因导致随机森林偏好数值型特征吗？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    10034&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">131. </span>
        <pre class="question_main">连续特征，既可以离散化，也可以做幅度缩放，那这两种处理方式分别适用于什么场景呢？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    10420&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">132. </span>
        <pre class="question_main">从几何直观的角度解释下为什么拉格朗日乘子法能取到最优值？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    9436&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">133. </span>
        <pre class="question_main">A/B测试的数学原理与深入理解</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    8583&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">134. </span>
        <pre class="question_main">如何更科学的做机器学习100天入门计划</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    3548&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">135. </span>
        <pre class="question_main">如何通俗理解主成成分分析PCA</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    4221&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">136. </span>
        <pre class="question_main">如何通俗理解LightGBM</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    12317&nbsp;浏览
                </li>
                <li>
                    4&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">137. </span>
        <pre class="question_main">线性回归要求因变量服从正态分布？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    4391&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">138. </span>
        <pre class="question_main">什么是K近邻算法和KD树？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    3317&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">139. </span>
        <pre class="question_main">如何通俗理解贝叶斯方法和贝叶斯网络？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    5164&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">140. </span>
        <pre class="question_main">最大熵模型中的数学推导</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    3247&nbsp;浏览
                </li>
                <li>
                    2&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">141. </span>
        <pre class="question_main">为什么在 xgboost 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值，一阶导数信息和二阶导数信息就可以进行叶子分裂优化计算？

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    3181&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">142. </span>
        <pre class="question_main">你有自己用过别的模型然后调参之类的吗？能说一下基本的调参流程吗？XGBoost知道吗，以XGBoost为例子说一下调参流程吧。

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2430&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">143. </span>
        <pre class="question_main">XGBoost和GBDT的区别有哪些？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2852&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">144. </span>
        <pre class="question_main">XGB特征重要性程度是怎么判断的？

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2547&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">145. </span>
        <pre class="question_main">xgb的预排序算法是怎么做的呢？

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2242&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">146. </span>
        <pre class="question_main">RF和xgboost哪个对异常点更敏感

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2128&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">147. </span>
        <pre class="question_main">xgb何时停止分裂？

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2331&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">148. </span>
        <pre class="question_main">对比一下XGB和lightGBM在节点分裂时候的区别

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2124&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">149. </span>
        <pre class="question_main">简要说一下Lightgbm相对于xgboost的优缺点

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2413&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">150. </span>
        <pre class="question_main">xgboost对特征缺失敏感吗，对缺失值做了什么操作，存在什么问题

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    3349&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">151. </span>
        <pre class="question_main">xgb和lgb在特征、数据并行上存在什么差异？

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2743&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">152. </span>
        <pre class="question_main">为什么xgboost不用后剪枝？

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    3298&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">153. </span>
        <pre class="question_main">为什么GBDT中叶子结点的分数是通过最小化损失函数得到的，而不是直接取平均或者多数表决？
</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    1601&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">154. </span>
        <pre class="question_main">SVM 和 Logistic 回归分别在什么情况下使用？</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2742&nbsp;浏览
                </li>
                <li>
                    1&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
    <div class="question_content">
        <div class="question_collect_back tiku_icon"><i class="tiku_icon question_collect"></i></div> <span
            class="question-num">155. </span>
        <pre class="question_main">Xgboost、lightGBM和Catboost之间的异同？ 

</pre> <!---->
        <div class="question-commonwrap">
            <ul class="question-commonlist">
                <li>
                    2739&nbsp;浏览
                </li>
                <li>
                    0&nbsp;评论
                </li>
            </ul>
        </div>
    </div>
</div>